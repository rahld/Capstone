rf_model <- randomForest(Revenue ~ Month_sin + Month_cos, data = data1, importance = TRUE)
data <- read.csv("./data/online_shoppers_intention.csv")
# 범주형 => 수치형
data1 <- data[, -c(12:15)]
data1$Month <- as.numeric(factor(data1$Month, levels = c("Jan", "Feb", "Mar", "Apr", "May", "June", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")))
View(data1)
data$VisitorType <- ifelse(data$VisitorType == "New_Visitor", 0, 1)
data <- read.csv("./data/online_shoppers_intention.csv")
# 범주형 => 수치형
data <- data[, -c(12:15)]
data$Month <- as.numeric(factor(data$Month, levels = c("Jan", "Feb", "Mar", "Apr", "May", "June", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")))
data$VisitorType <- ifelse(data$VisitorType == "New_Visitor", 0, 1)
data$Weekend <- ifelse(data$Weekend == FALSE, 0, 1)
unique_values <- unique(data$Revenue)
as.factor(data$Revenue)
# RandomForest
train_index <- sample(1:nrow(data), size = round(nrow(data) * 0.8))
train_set <- data[train_index, ]
test_set <- data[-train_index, ]
rf <- randomForest(as.factor(Revenue) ~., data=train_set, ntree = 500)
print(rf)
# RandomForest
set.seed(123)
# 변수중요도
importance(rf_model)
varImpPlot(rf_model)
# 변수중요도
importance(rf)
varImpPlot(rf)
# 2차 RF
rf <- randomForest(as.factor(Revenue) ~ PageValues + ExitRates + ProductRelated_Duration, data=train_set, ntree = 500)
print(rf)
# 변수중요도
importance(rf)
# 2차 RF
rf2 <- randomForest(as.factor(Revenue) ~ PageValues + ExitRates + ProductRelated_Duration, data=train_set, ntree = 500)
print(rf2)
rf <- randomForest(as.factor(Revenue) ~., data=train_set, ntree = 500)
print(rf)
# RandomForest
set.seed(123)
train_index <- sample(1:nrow(data), size = round(nrow(data) * 0.8))
train_set <- data[train_index, ]
test_set <- data[-train_index, ]
rf <- randomForest(as.factor(Revenue) ~., data=train_set, ntree = 500)
print(rf)
# 2차 RF
rf2 <- randomForest(as.factor(Revenue) ~ PageValues + ExitRates + ProductRelated_Duration, data=train_set, ntree = 500)
print(rf2)
# 3차 RF
rf3 <- randomForest(as.factor(Revenue) ~ PageValues +
ExitRates +
ProductRelated_Duration +
Administrative_Duration +
BounceRates +
Month_numeric +
VisitorType_numeric, data=train_set, ntree = 500)
# 3차 RF
rf3 <- randomForest(as.factor(Revenue) ~ PageValues +
ExitRates +
ProductRelated_Duration +
Administrative_Duration +
BounceRates +
Month +
VisitorType_numeric, data=train_set, ntree = 500)
# 3차 RF
rf3 <- randomForest(as.factor(Revenue) ~ PageValues +
ExitRates +
ProductRelated_Duration +
Administrative_Duration +
BounceRates +
Month +
VisitorType, data=train_set, ntree = 500)
print(rf3)
cor(data)
corplot(data)
corrplot(data)
# 상관분석
library(corrplot)
correlation_matrix <- cor(numeric_data, use = "complete.obs")
data1 <- ifelse(data$Revenue == FALSE, 0, 1)
correlation_matrix <- cor(data1, use = "complete.obs")
data1 <- data
data1 <- ifelse(data$Revenue == FALSE, 0, 1)
data1 <- data
data1$Revenue <- ifelse(data1$Revenue == FALSE, 0, 1)
View(data1)
correlation_matrix <- cor(data1, use = "complete.obs")
# 상관 행렬 시각화
corrplot(correlation_matrix, method = "circle",
type = "upper",
order = "hclust",
tl.col = "black",
tl.srt = 45,
addCoef.col = "black")
# 상관 행렬 시각화
corrplot(correlation_matrix, method = "circle",
type = "upper",
order = "hclust",
tl.col = "black",
tl.srt = 45,
tl.cex = 0.8,
number.cex = 0.5
addCoef.col = "black")
# 상관 행렬 시각화
corrplot(correlation_matrix, method = "circle",
type = "upper",
order = "hclust",
tl.col = "black",
tl.srt = 45,
tl.cex = 0.8,
number.cex = 0.5,
addCoef.col = "black")
# 상관 행렬 시각화
corrplot(correlation_matrix, method = "circle",
type = "upper",
order = "hclust",
tl.col = "black",
tl.srt = 45,
tl.cex = 0.5,
number.cex = 0.5,
addCoef.col = "black")
data2 <- data[data$ProductRelated_Duration < 40000, ]
Q1 <- quantile(data2$ProductRelated_Duration, 0.25)
Q3 <- quantile(data2$ProductRelated_Duration, 0.75)
IQR_value <- IQR(data2$ProductRelated_Duration)
lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value
outliers_data <- data2[data2$ProductRelated_Duration < lower_bound | data2$ProductRelated_Duration > 20000, ]
train_index2 <- sample(1:nrow(data2), size = round(nrow(data2) * 0.8))
train_set2 <- data2[train_index, ]
test_set2 <- data2[-train_index, ]
rf4 <- randomForest(as.factor(Revenue) ~ ., data=train_set2, ntree = 500)
library(tidyverse)
library(randomForest)
print(rf4)
View(rf3)
print(rf3)
print(rf2)
print(rf)
# 상관 행렬 시각화
corrplot(correlation_matrix, method = "circle",
type = "upper",
order = "hclust",
tl.col = "black",
tl.srt = 45,
tl.cex = 0.5,
number.cex = 0.5,
addCoef.col = "black")
# 상관분석
library(corrplot)
# 상관 행렬 시각화
corrplot(correlation_matrix, method = "circle",
type = "upper",
order = "hclust",
tl.col = "black",
tl.srt = 45,
tl.cex = 0.5,
number.cex = 0.5,
addCoef.col = "black")
data2
# 변수중요도
importance(rf)
varImpPlot(rf)
importance(rf2)
varImpPlot(rf2)
print(rf2)
importance(rf3)
varImpPlot(rf3)
train_index <- sample(1:nrow(data), size = round(nrow(data) * 0.8))
train_set <- data[train_index, ]
test_set <- data[-train_index, ]
rf <- randomForest(as.factor(Revenue) ~., data=train_set, ntree = 500)
print(rf)
# 2차 RF
rf2 <- randomForest(as.factor(Revenue) ~ PageValues + ExitRates + ProductRelated_Duration, data=train_set, ntree = 500)
print(rf2)
# 3차 RF
rf3 <- randomForest(as.factor(Revenue) ~ PageValues +
ExitRates +
ProductRelated_Duration +
Administrative_Duration +
BounceRates +
Month +
VisitorType, data=train_set, ntree = 500)
print(rf3)
train_index2 <- sample(1:nrow(data2), size = round(nrow(data2) * 0.8))
train_set2 <- data2[train_index, ]
test_set2 <- data2[-train_index, ]
rf4 <- randomForest(as.factor(Revenue) ~ ., data=train_set2, ntree = 500)
View(train_set2)
table(is.na(data2))
# 4 RF
data2$Revenue <- ifelse(data2$Revenue == FALSE, 0, 1)
train_index2 <- sample(1:nrow(data2), size = round(nrow(data2) * 0.8))
train_set2 <- data2[train_index, ]
test_set2 <- data2[-train_index, ]
rf4 <- randomForest(as.factor(Revenue) ~ ., data=train_set2, ntree = 500)
View(test_set)
View(data1)
View(data2)
View(outliers_data)
View(train_set)
View(data)
table(is.na(train_set2))
table(is.na(train_index2))
View(test_set2)
View(train_set2)
train_index2 <- sample(1:nrow(data2), size = round(nrow(data2) * 0.8))
train_set2 <- data2[train_index2, ]  # train_index2 사용
test_set2 <- data2[-train_index2, ]  # train_index2 사용
rf4 <- randomForest(as.factor(Revenue) ~ ., data = train_set2, ntree = 500)
print(rf4)
rf4 <- randomForest(as.factor(Revenue) ~ PageValues +
ExitRates +
ProductRelated_Duration +
Administrative_Duration +
BounceRates +
Month +
VisitorType, data = train_set2, ntree = 500)
print(rf4)
View(outliers_data)
View(outliers_data)
data <- data[data$ProductRelated_Duration < 20000, ]
# RandomForest
set.seed(123)
train_index <- sample(1:nrow(data), size = round(nrow(data) * 0.8))
train_set <- data[train_index, ]
test_set <- data[-train_index, ]
rf <- randomForest(as.factor(Revenue) ~., data=train_set, ntree = 500)
print(rf)
# 3차 RF
rf3 <- randomForest(as.factor(Revenue) ~ PageValues +
ExitRates +
ProductRelated_Duration +
Administrative_Duration +
BounceRates +
Month +
VisitorType, data=train_set, ntree = 500)
print(rf3)
importance(rf3)
varImpPlot(rf3)
predicted_rf3 <- predict(rf3, newdata = test_set)
# 혼동 행렬 생성
actual_rf3 <- test_set$Revenue
confusion_matrix_rf3 <- table(Predicted = predicted_rf3, Actual = actual_rf3)
print(confusion_matrix_rf3)
# 정확도 계산
accuracy_rf3 <- sum(diag(confusion_matrix_rf3)) / sum(confusion_matrix_rf3)
print(paste("Accuracy:", round(accuracy_rf3, 4)))
predicted_rf3
print(confusion_matrix_rf3)
precision <- confusion_matrix_rf3$byClass["Pos Pred Value"]  # Positive Predictive Value = Precision
library(caret)
predicted_rf3 <- predict(rf3, newdata = test_set)
# 혼동 행렬 생성
actual_rf3 <- test_set$Revenue
confusion_matrix_rf3 <- table(Predicted = predicted_rf3, Actual = actual_rf3)
print(confusion_matrix_rf3)
# 정확도 계산
accuracy_rf3 <- sum(diag(confusion_matrix_rf3)) / sum(confusion_matrix_rf3)
print(paste("Accuracy:", round(accuracy_rf3, 4)))
# 변수 중요도 확인 및 시각화
importance(rf3)
varImpPlot(rf3)
precision <- confusion_matrix_rf3$byClass["Pos Pred Value"]  # Positive Predictive Value = Precision
# 정밀도 (Precision) 계산: TP / (TP + FP)
precision <- confusion_matrix_rf3[2, 2] / sum(confusion_matrix_rf3[2, ])
# 재현율 (Recall) 계산: TP / (TP + FN)
recall <- confusion_matrix_rf3[2, 2] / sum(confusion_matrix_rf3[, 2])
# 결과 출력
cat("Precision:", round(precision, 4), "\n")
cat("Recall:", round(recall, 4), "\n")
# F1-스코어 계산
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1-Score:", round(f1_score, 4), "\n")
# XGBoost 패키지 설치 및 로드
install.packages("xgboost")
# 데이터 준비
train_set_xgb <- train_set
test_set_xgb <- test_set
# 데이터셋을 DMatrix 형식으로 변환
dtrain <- xgb.DMatrix(data = as.matrix(train_set_xgb[, -which(names(train_set_xgb) == "Revenue")]), label = train_set_xgb$Revenue)
library(xgboost)
# 데이터 준비
train_set_xgb <- train_set
test_set_xgb <- test_set
# 데이터셋을 DMatrix 형식으로 변환
dtrain <- xgb.DMatrix(data = as.matrix(train_set_xgb[, -which(names(train_set_xgb) == "Revenue")]), label = train_set_xgb$Revenue)
dtest <- xgb.DMatrix(data = as.matrix(test_set_xgb[, -which(names(test_set_xgb) == "Revenue")]), label = test_set_xgb$Revenue)
# XGBoost 모델 학습
params <- list(
objective = "binary:logistic",  # 이진 분류
eval_metric = "logloss",        # 평가 지표로 logloss 사용
scale_pos_weight = sum(train_set_xgb$Revenue == 0) / sum(train_set_xgb$Revenue == 1)  # 클래스 불균형 조정
)
# 학습
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 500)
# 예측
predictions_xgb <- predict(xgb_model, newdata = dtest)
# 예측 결과 이진화
predictions_xgb_binary <- ifelse(predictions_xgb > 0.5, 1, 0)
# 혼동 행렬
confusion_matrix_xgb <- table(Predicted = predictions_xgb_binary, Actual = test_set_xgb$Revenue)
print(confusion_matrix_xgb)
# 정밀도와 재현율 계산
precision_xgb <- confusion_matrix_xgb[2, 2] / sum(confusion_matrix_xgb[2, ])
recall_xgb <- confusion_matrix_xgb[2, 2] / sum(confusion_matrix_xgb[, 2])
# F1-스코어 계산
f1_score_xgb <- 2 * (precision_xgb * recall_xgb) / (precision_xgb + recall_xgb)
cat("Precision:", round(precision_xgb, 4), "\n")
cat("Recall:", round(recall_xgb, 4), "\n")
cat("F1-Score:", round(f1_score_xgb, 4), "\n")
# 필요한 라이브러리 로드
library(glmnet)
install.packages("glmnet")
# 필요한 라이브러리 로드
library(glmnet)
# 로지스틱 회귀 모델 학습
logistic_model <- glm(Revenue ~ PageValues + ExitRates + ProductRelated_Duration +
Administrative_Duration + BounceRates + Month + VisitorType,
data = train_set, family = "binomial")
# 모델 요약
summary(logistic_model)
# 테스트 데이터에서 예측
predicted_logistic <- predict(logistic_model, newdata = test_set, type = "response")
# 예측값을 0.5 임계값을 기준으로 이진화
predicted_logistic_binary <- ifelse(predicted_logistic > 0.5, 1, 0)
# 혼동 행렬 생성
confusion_matrix_logistic <- table(Predicted = predicted_logistic_binary, Actual = test_set$Revenue)
print(confusion_matrix_logistic)
# 정밀도와 재현율 계산
precision_logistic <- confusion_matrix_logistic[2, 2] / sum(confusion_matrix_logistic[2, ])
recall_logistic <- confusion_matrix_logistic[2, 2] / sum(confusion_matrix_logistic[, 2])
# F1-스코어 계산
f1_score_logistic <- 2 * (precision_logistic * recall_logistic) / (precision_logistic + recall_logistic)
cat("Precision:", round(precision_logistic, 4), "\n")
cat("Recall:", round(recall_logistic, 4), "\n")
cat("F1-Score:", round(f1_score_logistic, 4), "\n")
# 필요한 라이브러리 로드
library(class)
# KNN 모델 학습 (k = 5로 설정)
k <- 5
knn_model <- knn(train = train_set[, -which(names(train_set) == "Revenue")],
test = test_set[, -which(names(test_set) == "Revenue")],
cl = train_set$Revenue, k = k)
# 혼동 행렬 생성
confusion_matrix_knn <- table(Predicted = knn_model, Actual = test_set$Revenue)
print(confusion_matrix_knn)
# 정밀도와 재현율 계산
precision_knn <- confusion_matrix_knn[2, 2] / sum(confusion_matrix_knn[2, ])
recall_knn <- confusion_matrix_knn[2, 2] / sum(confusion_matrix_knn[, 2])
# F1-스코어 계산
f1_score_knn <- 2 * (precision_knn * recall_knn) / (precision_knn + recall_knn)
cat("Precision:", round(precision_knn, 4), "\n"
)
cat("Precision:", round(precision_knn, 4), "\n")
cat("Recall:", round(recall_knn, 4), "\n")
cat("F1-Score:", round(f1_score_knn, 4), "\n")
# KNN 모델 학습 (k = 5로 설정)
k <- 7
knn_model <- knn(train = train_set[, -which(names(train_set) == "Revenue")],
test = test_set[, -which(names(test_set) == "Revenue")],
cl = train_set$Revenue, k = k)
# 혼동 행렬 생성
confusion_matrix_knn <- table(Predicted = knn_model, Actual = test_set$Revenue)
print(confusion_matrix_knn)
# 정밀도와 재현율 계산
precision_knn <- confusion_matrix_knn[2, 2] / sum(confusion_matrix_knn[2, ])
recall_knn <- confusion_matrix_knn[2, 2] / sum(confusion_matrix_knn[, 2])
# F1-스코어 계산
f1_score_knn <- 2 * (precision_knn * recall_knn) / (precision_knn + recall_knn)
cat("Precision:", round(precision_knn, 4), "\n")
cat("Recall:", round(recall_knn, 4), "\n")
cat("F1-Score:", round(f1_score_knn, 4), "\n")
# KNN 모델 학습 (k = 5로 설정)
k <- 3
knn_model <- knn(train = train_set[, -which(names(train_set) == "Revenue")],
test = test_set[, -which(names(test_set) == "Revenue")],
cl = train_set$Revenue, k = k)
# 혼동 행렬 생성
confusion_matrix_knn <- table(Predicted = knn_model, Actual = test_set$Revenue)
print(confusion_matrix_knn)
# 정밀도와 재현율 계산
precision_knn <- confusion_matrix_knn[2, 2] / sum(confusion_matrix_knn[2, ])
recall_knn <- confusion_matrix_knn[2, 2] / sum(confusion_matrix_knn[, 2])
# F1-스코어 계산
f1_score_knn <- 2 * (precision_knn * recall_knn) / (precision_knn + recall_knn)
cat("Precision:", round(precision_knn, 4), "\n")
cat("Recall:", round(recall_knn, 4), "\n")
cat("F1-Score:", round(f1_score_knn, 4), "\n")
# KNN 모델 학습 (k = 5로 설정)
k <- 5
knn_model <- knn(train = train_set[, -which(names(train_set) == "Revenue")],
test = test_set[, -which(names(test_set) == "Revenue")],
cl = train_set$Revenue, k = k)
# 혼동 행렬 생성
confusion_matrix_knn <- table(Predicted = knn_model, Actual = test_set$Revenue)
print(confusion_matrix_knn)
# 정밀도와 재현율 계산
precision_knn <- confusion_matrix_knn[2, 2] / sum(confusion_matrix_knn[2, ])
recall_knn <- confusion_matrix_knn[2, 2] / sum(confusion_matrix_knn[, 2])
# F1-스코어 계산
f1_score_knn <- 2 * (precision_knn * recall_knn) / (precision_knn + recall_knn)
cat("Precision:", round(precision_knn, 4), "\n")
cat("Recall:", round(recall_knn, 4), "\n")
cat("F1-Score:", round(f1_score_knn, 4), "\n")
data <- read.csv("./data/online_shoppers_intention.csv")
# 범주형 => 수치형
data <- data[, -c(12:15)]
data$Month <- as.numeric(factor(data$Month, levels = c("Jan", "Feb", "Mar", "Apr", "May", "June", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")))
data$VisitorType <- ifelse(data$VisitorType == "New_Visitor", 0, 1)
data$Weekend <- ifelse(data$Weekend == FALSE, 0, 1)
# 결측값
table(is.na(data))
# 이상값
boxplot(data$ProductRelated_Duration)
Q1 <- quantile(data2$ProductRelated_Duration, 0.25)
Q3 <- quantile(data2$ProductRelated_Duration, 0.75)
IQR_value <- IQR(data2$ProductRelated_Duration)
lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value
data <- data[data$ProductRelated_Duration >= lower_bound & data$ProductRelated_Duration <= upper_bound, ]
unique_values <- unique(data$Revenue)
as.factor(data$Revenue)
data$Revenue <- ifelse(data1$Revenue == FALSE, 0, 1)
data$Revenue <- ifelse(data$Revenue == FALSE, 0, 1)
# 3차 RF (최최종)
set.seed(123)
rf3 <- randomForest(as.factor(Revenue) ~ PageValues +
ExitRates +
ProductRelated_Duration +
Administrative_Duration +
BounceRates +
Month +
VisitorType, data=train_set, ntree = 500)
print(rf3)
importance(rf3)
varImpPlot(rf3)
predicted_rf3 <- predict(rf3, newdata = test_set)
# 혼동 행렬 생성
actual_rf3 <- test_set$Revenue
confusion_matrix_rf3 <- table(Predicted = predicted_rf3, Actual = actual_rf3)
print(confusion_matrix_rf3)
# 정확도 계산
accuracy_rf3 <- sum(diag(confusion_matrix_rf3)) / sum(confusion_matrix_rf3)
print(paste("Accuracy:", round(accuracy_rf3, 4)))
# 3차 RF (최최종)
set.seed(123)
train_index <- sample(1:nrow(data), size = round(nrow(data) * 0.8))
train_set <- data[train_index, ]
test_set <- data[-train_index, ]
rf3 <- randomForest(as.factor(Revenue) ~ PageValues +
ExitRates +
ProductRelated_Duration +
Administrative_Duration +
BounceRates +
Month +
VisitorType, data=train_set, ntree = 500)
print(rf3)
importance(rf3)
varImpPlot(rf3)
predicted_rf3 <- predict(rf3, newdata = test_set)
# 혼동 행렬 생성
actual_rf3 <- test_set$Revenue
confusion_matrix_rf3 <- table(Predicted = predicted_rf3, Actual = actual_rf3)
print(confusion_matrix_rf3)
# 정확도 계산
accuracy_rf3 <- sum(diag(confusion_matrix_rf3)) / sum(confusion_matrix_rf3)
print(paste("Accuracy:", round(accuracy_rf3, 4)))
# 변수 중요도 확인 및 시각화
importance(rf3)
varImpPlot(rf3)
# 정밀도 (Precision) 계산: TP / (TP + FP)
precision <- confusion_matrix_rf3[2, 2] / sum(confusion_matrix_rf3[2, ])
# 재현율 (Recall) 계산: TP / (TP + FN)
recall <- confusion_matrix_rf3[2, 2] / sum(confusion_matrix_rf3[, 2])
# 결과 출력
cat("Precision:", round(precision, 4), "\n")
cat("Recall:", round(recall, 4), "\n")
# F1-스코어 계산
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1-Score:", round(f1_score, 4), "\n")
# 데이터 준비
train_set_xgb <- train_set
test_set_xgb <- test_set
# 데이터셋을 DMatrix 형식으로 변환
dtrain <- xgb.DMatrix(data = as.matrix(train_set_xgb[, -which(names(train_set_xgb) == "Revenue")]), label = train_set_xgb$Revenue)
dtest <- xgb.DMatrix(data = as.matrix(test_set_xgb[, -which(names(test_set_xgb) == "Revenue")]), label = test_set_xgb$Revenue)
# XGBoost 모델 학습
params <- list(
objective = "binary:logistic",  # 이진 분류
eval_metric = "logloss",        # 평가 지표로 logloss 사용
scale_pos_weight = sum(train_set_xgb$Revenue == 0) / sum(train_set_xgb$Revenue == 1)  # 클래스 불균형 조정
)
# 학습
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 500)
# 예측
predictions_xgb <- predict(xgb_model, newdata = dtest)
# 예측 결과 이진화
predictions_xgb_binary <- ifelse(predictions_xgb > 0.5, 1, 0)
# 혼동 행렬
confusion_matrix_xgb <- table(Predicted = predictions_xgb_binary, Actual = test_set_xgb$Revenue)
print(confusion_matrix_xgb)
# 정밀도와 재현율 계산
precision_xgb <- confusion_matrix_xgb[2, 2] / sum(confusion_matrix_xgb[2, ])
recall_xgb <- confusion_matrix_xgb[2, 2] / sum(confusion_matrix_xgb[, 2])
# F1-스코어 계산
f1_score_xgb <- 2 * (precision_xgb * recall_xgb) / (precision_xgb + recall_xgb)
cat("Precision:", round(precision_xgb, 4), "\n")
cat("Recall:", round(recall_xgb, 4), "\n")
cat("F1-Score:", round(f1_score_xgb, 4), "\n")
